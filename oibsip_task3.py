# -*- coding: utf-8 -*-
"""OIBSIP_Task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bK2hYPaUFYJVnx47yvuZPJbGLW48SEyT
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd

df = pd.read_csv('car data.csv')
df

print(df.info())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

# Drop the Car_Name column as it's unlikely to be a useful predictor
df.drop(columns=['Car_Name'], inplace=True)

# Separate features and target variable
X = df.drop(columns=['Selling_Price'])
y = df['Selling_Price']

# Preprocess the data
# Define categorical and numerical columns
categorical_cols = ['Fuel_Type', 'Selling_type', 'Transmission']
numerical_cols = ['Year', 'Present_Price', 'Driven_kms', 'Owner']

# Preprocessing for numerical data
numerical_transformer = StandardScaler()

# Preprocessing for categorical data
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Define the model
model = LinearRegression()

# Create and evaluate the pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', model)
                          ])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'Mean Absolute Error: {mae}')

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting the distributions of numerical features
plt.figure(figsize=(12, 6))

plt.subplot(2, 2, 1)
sns.histplot(df['Year'], kde=True, bins=15)
plt.title('Distribution of Year')

plt.subplot(2, 2, 2)
sns.histplot(df['Present_Price'], kde=True, bins=15)
plt.title('Distribution of Present Price')

plt.subplot(2, 2, 3)
sns.histplot(df['Driven_kms'], kde=True, bins=15)
plt.title('Distribution of Driven Kilometers')

plt.subplot(2, 2, 4)
sns.histplot(df['Owner'], kde=True, bins=15)
plt.title('Distribution of Owner')

plt.tight_layout()
plt.show()

# Plotting the distribution of the target variable
plt.figure(figsize=(6, 4))
sns.histplot(df['Selling_Price'], kde=True, bins=15)
plt.title('Distribution of Selling Price')
plt.show()

# Pairplot to see relationships between variables
sns.pairplot(df)
plt.show()

# Plot actual vs. predicted values for Linear Regression
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.title('Actual vs Predicted Selling Price (Linear Regression)')
plt.show()

# Create and evaluate the pipeline with Random Forest
rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', RandomForestRegressor(n_estimators=100, random_state=42))
                             ])

rf_pipeline.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_pipeline.predict(X_test)

# Plot actual vs. predicted values for Random Forest
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red')
plt.xlabel('Actual Selling Price')
plt.ylabel('Predicted Selling Price')
plt.title('Actual vs Predicted Selling Price (Random Forest)')
plt.show()